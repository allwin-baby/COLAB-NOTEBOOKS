{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MY implimetation of neural network.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMNjCgBqwEpRDKmtFEfyp2Z"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"tiMbcltix9Mo","colab_type":"code","colab":{}},"source":["import numpy as np\n","import math\n","import math\n","\n","\n","def sigmoid(Z):\n","    \"\"\"\n","    Implements the sigmoid activation in numpy\n","    \n","    Arguments:\n","    Z -- numpy array of any shape\n","    \n","    Returns:\n","    A -- output of sigmoid(z), same shape as Z\n","    cache -- returns Z as well, useful during backpropagation\n","    \"\"\"\n","    \n","    A = 1/(1+np.exp(-Z))\n","    cache = Z\n","    \n","    return A, cache\n","\n","def relu(Z):\n","    \"\"\"\n","    Implement the RELU function.\n","\n","    Arguments:\n","    Z -- Output of the linear layer, of any shape\n","\n","    Returns:\n","    A -- Post-activation parameter, of the same shape as Z\n","    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    A = np.maximum(0,Z)\n","    \n","    assert(A.shape == Z.shape)\n","    \n","    cache = Z \n","    return A, cache\n","\n","\n","def relu_backward(dA, cache):\n","    \"\"\"\n","    Implement the backward propagation for a single RELU unit.\n","\n","    Arguments:\n","    dA -- post-activation gradient, of any shape\n","    cache -- 'Z' where we store for computing backward propagation efficiently\n","\n","    Returns:\n","    dZ -- Gradient of the cost with respect to Z\n","    \"\"\"\n","    \n","    Z = cache\n","    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n","    \n","    # When z <= 0, you should set dz to 0 as well. \n","    dZ[Z <= 0] = 0\n","    \n","    assert (dZ.shape == Z.shape)\n","    \n","    return dZ\n","\n","def sigmoid_backward(dA, cache):\n","    \"\"\"\n","    Implement the backward propagation for a single SIGMOID unit.\n","\n","    Arguments:\n","    dA -- post-activation gradient, of any shape\n","    cache -- 'Z' where we store for computing backward propagation efficiently\n","\n","    Returns:\n","    dZ -- Gradient of the cost with respect to Z\n","    \"\"\"\n","    \n","    Z = cache\n","    \n","    s = 1/(1+np.exp(-Z))\n","    dZ = dA * s * (1-s)\n","    \n","    assert (dZ.shape == Z.shape)\n","    \n","    return dZ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K-XlsJglQRUJ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OzDxpDRf5cXW","colab_type":"code","colab":{}},"source":["W1 = np.array([[ 0.8, 0.4], [0.3 , 0.2],[0.9, 0.5 ]])\n","b1 = np.array([[ 0],[ 0],[0]])\n","W2 = np.array([[ 0.3, 0.5, 0.9]])\n","b2 = np.array([[0]])\n","x = np.array([[1],[1]])\n","Y= np.array([[1],[1]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L1dn_BRx2Epc","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R3XPHQzt0gxL","colab_type":"code","colab":{}},"source":["def linear_forward(A, W, b):\n","    \"\"\"\n","    Implement the linear part of a layer's forward propagation.\n","\n","    Arguments:\n","    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","\n","    Returns:\n","    Z -- the input of the activation function, also called pre-activation parameter \n","    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 1 line of code)\n","    Z = np.dot(W,A)+b      #only reprecenatational coding\n","    ### END CODE HERE ###\n","    \n","    assert(Z.shape == (W.shape[0], A.shape[1]))\n","    cache = (A, W, b)\n","    \n","    return Z, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kPmBqJbM3bue","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1599169920241,"user_tz":-330,"elapsed":2836,"user":{"displayName":"allwin baby","photoUrl":"","userId":"15788187532421881332"}},"outputId":"b2807334-a1d0-40c8-e56d-e6f8f2881b5e"},"source":["\n","Z, linear_cache = linear_forward(x, W1, b1)\n","print(\"Z = \" + str(Z))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Z = [[1.2]\n"," [0.5]\n"," [1.4]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7g8LyoVz26Sy","colab_type":"code","colab":{}},"source":["def linear_forward(A, W, b):\n","    \"\"\"\n","    Implement the linear part of a layer's forward propagation.\n","\n","    Arguments:\n","    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","\n","    Returns:\n","    Z -- the input of the activation function, also called pre-activation parameter \n","    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    ### START CODE HERE ### (≈ 1 line of code)\n","    Z = np.dot(W,A)+b      #only reprecenatational coding\n","    ### END CODE HERE ###\n","    \n","    assert(Z.shape == (W.shape[0], A.shape[1]))\n","    cache = (A, W, b)\n","    \n","    return Z, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1qFdDQCP90lD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1599170537426,"user_tz":-330,"elapsed":1291,"user":{"displayName":"allwin baby","photoUrl":"","userId":"15788187532421881332"}},"outputId":"ac8ec522-c487-4a25-819c-a868796e6e95"},"source":["Z, linear_cache = linear_forward(x, W1, b1)\n","print(\"Z = \" + str(Z))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Z = [[1.2]\n"," [0.5]\n"," [1.4]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HDWddiwh-Zpv","colab_type":"code","colab":{}},"source":["def linear_activation_forward(A_prev, W, b, activation):\n","    \"\"\"\n","    Implement the forward propagation for the LINEAR->ACTIVATION layer\n","\n","    Arguments:\n","    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","\n","    Returns:\n","    A -- the output of the activation function, also called the post-activation value \n","    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n","             stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    if activation == \"sigmoid\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        Z, linear_cache = linear_forward(A_prev, W, b)  #predefined in same  page\n","        A, activation_cache = sigmoid(Z)\n","        ### END CODE HERE ###\n","    \n","    elif activation == \"relu\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","        ### START CODE HERE ### (≈ 2 lines of code)\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","        ### END CODE HERE ###\n","    \n","    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n","    cache = (linear_cache, activation_cache)\n","\n","    return A, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K9yF2j6t_lhE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"executionInfo":{"status":"ok","timestamp":1599171664205,"user_tz":-330,"elapsed":990,"user":{"displayName":"allwin baby","photoUrl":"","userId":"15788187532421881332"}},"outputId":"1156661e-4c41-4542-82f3-b8de565c174b"},"source":["A, linear_activation_cache = linear_activation_forward(x, W1, b1, activation = \"sigmoid\")\n","AL, linear_activation_cache = linear_activation_forward(A, W2, b2, activation = \"sigmoid\")\n","print(\"With sigmoid: A = \" + str(A))\n","print(\"With sigmoid: A = \" + str(AL))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["With sigmoid: A = [[0.76852478]\n"," [0.62245933]\n"," [0.80218389]]\n","With sigmoid: A = [[0.77967142]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"f7Y_yVEr_337","colab_type":"code","colab":{}},"source":["def compute_cost(AL, Y):\n","    \"\"\"\n","    Implement the cost function defined by equation (7).\n","\n","    Arguments:\n","    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n","    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n","\n","    Returns:\n","    cost -- cross-entropy cost\n","    \"\"\"\n","    \n","    m = Y.shape[1]\n","\n","    # Compute loss from aL and y.\n","    ### START CODE HERE ### (≈ 1 lines of code)\n","    cost = -1/m*np.sum(np.multiply(np.log(AL),Y) +  np.multiply(np.log(1-AL), (1-Y)))\n","    ### END CODE HERE ###\n","    \n","    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n","    assert(cost.shape == ())\n","    \n","    return cost"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MbRUx6skCCuQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599171967024,"user_tz":-330,"elapsed":2023,"user":{"displayName":"allwin baby","photoUrl":"","userId":"15788187532421881332"}},"outputId":"8f634f71-c0cf-4dcd-a479-13f903b7eab5"},"source":["print(\"cost = \" + str(compute_cost(AL, Y)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cost = 0.4977654074214332\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jo2wmNFbD-Ob","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}